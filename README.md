# MSI_PROJECT ‚Äì Deep Learning for Waste Classification

## üìÅ Project Structure

```
MSI_PROJECT/
‚îÇ
‚îú‚îÄ‚îÄ images/                     # Original (raw) dataset
‚îÇ   ‚îú‚îÄ‚îÄ cardboard/
‚îÇ   ‚îú‚îÄ‚îÄ glass/    
‚îÇ   ‚îú‚îÄ‚îÄ metal/    
‚îÇ   ‚îú‚îÄ‚îÄ paper/    
‚îÇ   ‚îú‚îÄ‚îÄ plastic/  
‚îÇ   ‚îî‚îÄ‚îÄ trash/    
‚îÇ
‚îú‚îÄ‚îÄ augmented/                  # Automatically generated augmented images
‚îÇ   ‚îú‚îÄ‚îÄ cardboard/
‚îÇ   ‚îú‚îÄ‚îÄ glass/    
‚îÇ   ‚îú‚îÄ‚îÄ metal/    
‚îÇ   ‚îú‚îÄ‚îÄ paper/    
‚îÇ   ‚îú‚îÄ‚îÄ plastic/  
‚îÇ   ‚îî‚îÄ‚îÄ trash/    
‚îÇ
‚îú‚îÄ‚îÄ extracted_features/         # Extracted features (numpy arrays)     
‚îÇ   ‚îú‚îÄ‚îÄ X.npy                   # PCA-reduced features
‚îÇ   ‚îú‚îÄ‚îÄ X_raw.npy               # Raw CNN embeddings
‚îÇ   ‚îú‚îÄ‚îÄ y.npy                   # Labels
‚îÇ   ‚îú‚îÄ‚îÄ scaler.pkl              # StandardScaler model
‚îÇ   ‚îú‚îÄ‚îÄ pca.pkl                 # PCA model
‚îÇ   ‚îî‚îÄ‚îÄ class_map.txt           # Class index mapping
‚îÇ
‚îú‚îÄ‚îÄ *.py                        # Python scripts for the project
‚îÇ
‚îî‚îÄ‚îÄ venv/                       # (Optional) Python virtual environment
```

---

## üìò Executive Summary

This project implements a complete Machine Learning pipeline for automated waste classification using deep learning and traditional classifiers. The system achieves exceptional accuracy (99.73% for K-Nearest Neighbors and 99.87% for Support Vector Machine) through a three-stage architecture: data augmentation to increase dataset diversity, CNN-based feature extraction using MobileNetV2 to capture high-level visual representations, and classifier training for real-time deployment with live camera feed.

### Key Performance Metrics

| Metric | Value |
|--------|-------|
| **Dataset Size (Original)** | 1,960 images across 6 classes |
| **Dataset Size (Augmented)** | 7,460 images (3.8√ó expansion) |
| **Feature Dimensions (Raw)** | 1,286 (MobileNetV2 embeddings + color statistics) |
| **Feature Dimensions (PCA)** | 200 (75.92% variance retained) |
| **KNN Accuracy** | 99.73% |
| **SVM Accuracy** | 99.87% |
| **Inference Speed** | 5-10 frames per second |

---

## üìÇ File 1: `data_augmentation.py`

### Overview and Purpose

The data augmentation module addresses the fundamental challenge of limited training data by artificially expanding the dataset size while introducing controlled variance. The original dataset contained 1,960 images distributed across six waste categories with significant class imbalance, particularly in the trash category which represented only 110 images. Through systematic geometric and photometric transformations, this module generates 7,460 total images, providing the downstream deep learning model with sufficient diversity to learn robust feature representations while preventing overfitting to the limited original samples.

### Workflow and Logic

The augmentation pipeline operates through a structured four-stage process. First, the system validates that input directories exist and creates corresponding output directories with matching class folder structures to preserve label integrity. Second, it iterates through each class folder and processes every image individually. Third, each image undergoes immediate resizing to 224√ó224 pixels to ensure dimensional consistency required by MobileNetV2 architecture. Fourth, the augmentation engine generates three distinct variations through horizontal flipping, random rotation within safe angular bounds, and brightness adjustment in HSV color space. Finally, both the original resized image and its three augmented variants are saved to disk, resulting in a four-fold increase in dataset size per original image.

### Core Functions and Implementation

The `augment_image()` function serves as the transformation engine that generates three variations from a single input image. The horizontal flip operation uses OpenCV's flip function with axis parameter set to 1, creating mirror images that preserve object semantics since waste items typically exhibit vertical symmetry regardless of left-right orientation. The rotation operation calculates a transformation matrix for a random angle between negative fifteen and positive fifteen degrees, applying it with linear interpolation and reflective border handling. This angular constraint prevents excessive distortion that could compromise object recognizability while simulating natural camera tilt variations encountered in real-world deployment. The brightness adjustment converts images from BGR to HSV color space, scales the Value channel by a factor between 0.8 and 1.2, then converts back to BGR. This approach preserves color information (Hue and Saturation channels) while varying overall lightness, effectively simulating different lighting conditions without washing out color fidelity.

The `augment_dataset()` function manages the end-to-end pipeline execution, handling file input and output operations while tracking progress across all classes. It mirrors the folder structure of the input directory into the output directory to ensure label preservation throughout the augmentation process.

### Configuration Parameters

The image size parameter is set to 224√ó224 pixels, which represents the standard input dimension for MobileNetV2 architecture as defined in the original paper. This resolution provides sufficient detail for the convolutional neural network to extract meaningful features while maintaining computational efficiency for CPU-based processing. The rotation range is constrained to ¬±15 degrees to maintain object integrity. Empirical testing has demonstrated that larger rotations introduce excessive black border artifacts and can distort object context in ways that confuse the classifier. The brightness factor varies randomly between 0.8 and 1.2, providing 20% darkening or brightening to simulate varied lighting conditions encountered across different times of day and environments. The linear interpolation flag ensures smooth pixel transitions during geometric transformations, balancing processing speed with output quality. The reflective border mode fills empty corners created during rotation by mirroring edge pixels rather than introducing black artifacts that could mislead the neural network during feature learning.

### Dataset Statistics

The augmentation process transformed the original dataset from 1,960 images to 7,460 images. The class distribution after augmentation shows cardboard with 988 images, glass with 1,540 images, metal with 1,260 images, paper with 1,796 images, plastic with 1,452 images, and trash with 424 images. This expansion addresses the critical challenge of limited training data while maintaining the relative class proportions necessary for effective model training.

---

## üìÇ File 2: `feature_extraction_mobilenet.py`

### Overview and Purpose

This module represents a fundamental shift from traditional computer vision techniques to deep learning-based feature extraction. The system leverages MobileNetV2, a convolutional neural network pretrained on ImageNet containing 1.2 million images across 1,000 categories. Rather than manually engineering features through histogram-based descriptors, this approach utilizes transfer learning to extract high-level semantic representations that the network learned during pretraining. MobileNetV2 was specifically selected for its efficiency-accuracy tradeoff, utilizing depthwise separable convolutions that dramatically reduce computational requirements compared to standard convolutional architectures while maintaining strong performance. This makes the network feasible for CPU-only inference on standard laptops without requiring GPU acceleration.

### Workflow and Logic

The feature extraction pipeline operates through six distinct stages. First, the system loads all augmented images from disk and assigns numeric labels based on their parent folder names, creating a complete dataset representation in memory. Second, it instantiates the MobileNetV2 architecture with ImageNet weights, excluding the classification head and utilizing global average pooling to produce fixed-length feature vectors. Third, images are processed in batches to optimize memory usage and processing efficiency. Each image undergoes color space conversion from BGR to RGB, resizing to 224√ó224 pixels, and MobileNetV2-specific preprocessing that normalizes pixel values according to ImageNet training statistics. Fourth, the network performs forward propagation to generate 1,280-dimensional embedding vectors representing learned visual features. Fifth, LAB color statistics are extracted and concatenated with CNN embeddings to incorporate explicit color information that complements the learned representations. Sixth, the combined 1,286-dimensional feature vectors undergo standardization through StandardScaler to normalize value ranges, followed by Principal Component Analysis dimensionality reduction to compress the representation to 200 dimensions while retaining 75.92% of the original variance.

### Architecture and Design Decisions

The MobileNetV2 architecture employs inverted residual blocks with linear bottlenecks, a design that significantly reduces computational cost while preserving representational capacity. The network processes input images through a series of convolutional layers with progressively increasing feature channel counts, capturing hierarchical visual patterns from low-level edges and textures to high-level object parts and semantic concepts. By utilizing the pretrained ImageNet weights, the model benefits from learned representations that generalize well to waste classification despite the domain difference, as fundamental visual patterns such as shapes, materials, and textures remain consistent across domains. The global average pooling layer aggregates spatial information into a single 1,280-dimensional vector, effectively summarizing the entire image into a compact representation suitable for classification.

### Core Functions and Implementation

The `build_mobilenet()` function instantiates the MobileNetV2 model with specific configuration parameters. The `include_top` parameter is set to False to remove the ImageNet classification head, as the goal is feature extraction rather than 1,000-class classification. The `weights` parameter specifies "imagenet" to load pretrained parameters learned from large-scale visual recognition. The `input_shape` defines the expected tensor dimensions as 224√ó224√ó3 corresponding to RGB images. The `pooling` parameter set to "avg" applies global average pooling rather than flattening, producing a more compact representation that reduces subsequent classifier complexity. The model is marked as non-trainable since the goal is leveraging learned features rather than fine-tuning the network weights.

The `lab_stats()` function extracts color distribution statistics that complement the CNN embeddings. It converts images from BGR to LAB color space, which separates luminance (L channel) from color information (A and B channels), providing a perceptually uniform color representation. The function calculates mean and standard deviation across all pixels for each of the three channels, producing a six-dimensional vector that captures overall color characteristics such as brightness level, green-red balance, and blue-yellow balance.

The `load_images_from_dir()` function handles dataset loading by traversing the augmented directory structure and associating each image with its corresponding class label. It creates a mapping dictionary that assigns integer indices to class names alphabetically, ensuring consistent label encoding throughout the pipeline.

The `extract_and_save()` function orchestrates the complete feature extraction workflow. Images are processed in batches to balance memory consumption with processing efficiency. For each batch, images undergo preprocessing including color space conversion, resizing, and normalization according to MobileNetV2 requirements. The CNN forward pass generates embeddings, which are then concatenated with LAB color statistics to form the complete feature representation. After processing all images, the raw 1,286-dimensional features are standardized using StandardScaler to normalize each feature dimension to zero mean and unit variance, ensuring all features contribute equally to subsequent distance-based classification. Principal Component Analysis then reduces dimensionality to 200 components, compressing the representation while retaining the most significant variance patterns.

### Configuration Parameters and Rationale

The data directory points to the augmented dataset location, maintaining pipeline consistency with the augmentation module output. The output directory specifies where processed features and trained preprocessing models will be saved for subsequent use by classification scripts. The backbone selection is fixed to MobileNetV2 based on its optimal balance of accuracy and computational efficiency for CPU inference. The image size of 224√ó224 pixels aligns with the standard input dimensions used during MobileNetV2 training on ImageNet, ensuring the network receives inputs in the expected format.

The color statistics flag is enabled to append LAB color features to CNN embeddings. While MobileNetV2 learns color-sensitive representations through its training process, explicitly including color statistics provides complementary information that can improve classification performance, particularly for waste categories where color is a distinguishing characteristic such as brown cardboard versus transparent plastic. The PCA target dimension is set to 200 components, representing a reduction from 1,286 dimensions while retaining approximately 76% of the variance. This compression removes noise and redundant information while maintaining the most discriminative patterns, improving classifier generalization and reducing computational requirements during training and inference.

The batch size of 16 images represents a compromise between processing efficiency and memory constraints. Larger batches would improve GPU utilization but exceed available memory on standard laptops, while smaller batches would underutilize vectorization optimizations. The random state parameter ensures reproducibility of the PCA transformation across multiple runs, critical for consistent pipeline behavior during development and deployment.

### Transfer Learning Rationale

The decision to employ transfer learning through pretrained MobileNetV2 rather than training a custom network from scratch addresses several fundamental challenges. First, the augmented dataset of 7,460 images remains relatively small for training deep convolutional networks, which typically require hundreds of thousands or millions of samples to learn effective representations. Second, ImageNet pretraining provides the network with learned visual primitives such as edge detectors, texture analyzers, and shape recognizers that generalize across domains. Third, the computational cost of training deep networks from scratch would be prohibitive on CPU-only hardware, whereas using pretrained weights enables immediate feature extraction capability.

The performance improvement from traditional computer vision features to deep learning embeddings demonstrates the effectiveness of this approach. The previous pipeline utilizing Histogram of Oriented Gradients, Local Binary Patterns, Gray-Level Co-occurrence Matrix statistics, and Bag of Visual Words achieved 86.86% accuracy for K-Nearest Neighbors and 88.54% for Support Vector Machine. The current MobileNetV2-based approach achieves 99.73% and 99.87% respectively, representing an improvement of approximately 13 percentage points. This dramatic increase validates the superiority of learned representations over handcrafted features for this classification task.

---

## üìÇ File 3: `knn_training.py`

### Overview and Purpose

The K-Nearest Neighbors training module implements a non-parametric classification approach that makes predictions based on local similarity in feature space. The algorithm assigns class labels by identifying the K nearest training samples to a query point and selecting the majority class among those neighbors. This module loads the MobileNetV2-extracted features, performs stratified train-test splitting to ensure representative evaluation, trains or loads a KNN classifier with optimized hyperparameters, and generates comprehensive visualizations to facilitate model understanding and performance analysis.

### Workflow and Logic

The training workflow follows a systematic six-stage process. First, the system loads preprocessed features from the feature extraction output directory, including the 200-dimensional PCA-reduced feature vectors and their corresponding class labels. Second, it reads the class name mapping from the text file to enable human-readable reporting throughout the analysis. Third, a stratified split divides the data into 80% training and 20% testing subsets while maintaining the original class distribution in both partitions. Fourth, the system either loads an existing trained model or instantiates a new KNN classifier with specified hyperparameters and fits it to the training data. Fifth, predictions are generated for the test set to evaluate model performance through overall accuracy and per-class accuracy metrics. Sixth, comprehensive visualizations are created showing class distribution, PCA projections, confusion matrix, and prediction correctness to provide intuitive understanding of model behavior.

### Stratified Splitting Rationale

The `StratifiedShuffleSplit` function ensures that the train-test division maintains proportional representation of all classes. Without stratification, random splitting could inadvertently allocate disproportionate class samples to training versus testing sets, particularly problematic for the imbalanced trash category. Stratified splitting guarantees that if cardboard represents 13.2% of the total dataset, it will represent exactly 13.2% of both training and testing subsets. This produces more reliable accuracy estimates and prevents evaluation bias that could arise from underrepresenting minority classes in the test set.

### Hyperparameter Configuration

The number of neighbors parameter is set to 7, representing the optimal value identified through systematic grid search evaluation. Smaller values such as 3 or 5 make the model overly sensitive to local noise and outliers, as a single mislabeled training sample can disproportionately influence predictions. Larger values such as 15 or 21 average across too many samples, causing the model to lose sensitivity to local patterns and potentially misclassifying boundary cases. The value of 7 provides the optimal balance between noise robustness and local pattern sensitivity, achieving 99.73% accuracy on the test set.

The weights parameter is configured to "distance" rather than uniform weighting. This means that closer neighbors have greater influence on the classification decision than distant neighbors, implementing an inverse distance weighting scheme. This prevents situations where a query point lies very close to samples from one class but the K nearest neighbors include several distant samples from another class, which could incorrectly sway the majority vote under uniform weighting.

The metric parameter specifies "cosine" distance rather than Euclidean or Manhattan distance. Cosine similarity measures the angle between feature vectors rather than their absolute magnitude, making it particularly suitable for high-dimensional embedding spaces where vector length can vary due to image properties such as brightness while directional alignment captures semantic similarity. This property makes cosine distance more robust to scale variations in the feature space.

The algorithm parameter set to "auto" allows scikit-learn to select the optimal nearest neighbor search algorithm based on data characteristics. For moderate-sized datasets, the library typically employs ball tree or KD tree structures that enable efficient logarithmic-time neighbor queries rather than brute-force linear search across all training samples.

### Visualization Components

The comprehensive visualization suite generates six distinct plots that facilitate model interpretation. The class distribution plot displays sample counts per category as a bar chart, revealing the dataset imbalance where paper has the most samples and trash has the fewest. The PCA two-dimensional projection plots all samples in reduced feature space, showing whether classes form distinct clusters or exhibit overlap that could challenge classification. The three-dimensional PCA projection provides an additional perspective on feature space structure and class separability. The train-test split visualization distinguishes training samples with circles and testing samples with squares, confirming that both sets span the feature space appropriately. The confusion matrix displays actual versus predicted class labels in a heatmap format, revealing which class pairs are most frequently confused. The predictions plot highlights correctly classified test samples with green borders and misclassified samples with red X markers, providing intuitive feedback on model performance patterns.

---

## üìÇ File 4: `svm_training.py`

### Overview and Purpose

The Support Vector Machine training module implements a powerful classification algorithm that seeks the optimal separating hyperplane between classes in feature space. Unlike K-Nearest Neighbors which memorizes training data, Support Vector Machine learns a decision function that generalizes beyond the training set by maximizing the margin between class boundaries. The workflow architecture mirrors the KNN module exactly to ensure consistent evaluation methodology and fair performance comparison between the two approaches.

### Algorithm Fundamentals

Support Vector Machine operates by identifying support vectors, which are the training samples that lie closest to the decision boundary between classes. The algorithm constructs a hyperplane that maximizes the perpendicular distance (margin) from the nearest samples of each class. For linearly separable problems, this produces the decision boundary with maximum generalization capability. For non-linearly separable problems, which is typical in real-world applications, the kernel trick maps the original feature space into a higher-dimensional space where linear separation becomes possible.

### Hyperparameter Configuration

The kernel parameter is set to Radial Basis Function rather than linear, polynomial, or sigmoid alternatives. RBF kernel measures similarity between samples using Gaussian functions, effectively creating circular or elliptical decision boundaries in the original feature space. This enables the model to capture complex non-linear patterns that characterize waste classification, where feature distributions rarely form linearly separable clusters. The RBF kernel was selected through systematic evaluation that demonstrated superior performance over alternative kernel types, achieving 99.87% accuracy.

The C parameter controls the regularization strength, representing the penalty for misclassifying training samples. A value of 10 provides optimal balance between fitting the training data and maintaining decision boundary smoothness. Low C values such as 0.1 create soft margins that tolerate training errors in favor of simplicity, potentially underfitting the data. High C values such as 100 or 200 create hard margins that minimize training error at the risk of overfitting to noise. The value of 10 was identified through cross-validation as providing the best generalization performance.

The gamma parameter determines the influence radius of individual training samples when using the RBF kernel. The "scale" setting automatically computes gamma as the inverse of the product of feature count and feature variance, adapting to the natural spread of the data. High gamma values make the model focus only on nearby samples, creating complex decision boundaries that risk overfitting. Low gamma values extend influence across distant samples, potentially underfit by oversimplifying boundaries. The automatic scaling ensures appropriate behavior without manual tuning.

The probability parameter is enabled to generate confidence estimates alongside class predictions. This requires additional calibration that converts the Support Vector Machine decision function values into well-calibrated probability distributions through Platt scaling. These probabilities are essential for the real-time inference system, enabling confidence thresholding that rejects ambiguous predictions below a specified threshold.

### Performance Analysis

Support Vector Machine achieves 99.87% test accuracy, slightly outperforming K-Nearest Neighbors at 99.73%. This 0.14 percentage point difference reflects the fundamental algorithmic properties of each approach. Support Vector Machine learns a global decision function optimized for maximum margin, making it robust to local noise and outliers that might affect K-Nearest Neighbors. The smooth decision boundaries created by the RBF kernel can better capture the continuous nature of CNN embedding spaces compared to the piecewise linear boundaries created by majority voting in K-Nearest Neighbors. However, both models achieve exceptional performance exceeding 99% accuracy, demonstrating that the MobileNetV2 feature representations provide highly discriminative information that enables accurate classification with either approach.

---

## üìÇ File 5: `input.py`

### Overview and Purpose

The real-time inference module bridges the gap between offline model training and practical deployment by enabling live waste classification through webcam feed. The system captures video frames continuously, applies identical preprocessing and feature extraction pipelines used during training, generates predictions from both KNN and SVM classifiers, and displays annotated results with classification labels and confidence scores. The critical challenge addressed by this module is maintaining perfect consistency between training and inference pipelines to prevent feature distribution shifts that would degrade model performance.

### Workflow and Logic

The inference pipeline executes through a continuous capture-process-display loop. During initialization, the system loads both trained classifiers along with all preprocessing artifacts including the scaler model, PCA model, and MobileNetV2 architecture with ImageNet weights. The webcam is initialized to capture frames from the default video device. In the main loop, each frame is captured and passed to the feature extraction function which performs identical processing steps as training. First, color space conversion from BGR to RGB ensures compatibility with MobileNetV2 preprocessing. Second, resizing to 224√ó224 pixels matches the network input dimensions. Third, the frame is passed through MobileNetV2 to generate 1,280-dimensional embeddings. Fourth, LAB color statistics are extracted and concatenated with CNN features. Fifth, the combined 1,286-dimensional vector is transformed using the pretrained scaler to normalize feature ranges. Sixth, PCA reduction compresses the features to 200 dimensions. Seventh, both classifiers generate probability distributions over the six waste categories. Eighth, confidence thresholding determines whether to display the predicted class or mark the prediction as unknown if confidence falls below 60%. Finally, predictions are overlaid on the video frame and displayed in real-time.

### Feature Extraction Consistency

The `extract_cnn_features()` function replicates the exact preprocessing sequence applied during training. Color space conversion from BGR to RGB is essential because OpenCV captures frames in BGR format while MobileNetV2 expects RGB format matching ImageNet training data. Omitting this conversion would cause systematic color channel misalignment that degrades prediction accuracy. The resizing operation uses linear interpolation to match the resize method employed during training, ensuring pixel value distributions remain consistent. The MobileNetV2 preprocessing function applies the same normalization that centers and scales pixel values according to ImageNet statistics.

The LAB color statistics extraction must precisely match the training pipeline implementation. The same channel order (L, A, B), same statistical calculations (mean and standard deviation per channel), and same concatenation order ensure that the resulting six-dimensional vector occupies the same feature space positions as during training.

### Transform Versus Fit-Transform

A critical implementation detail is the use of `transform()` rather than `fit_transform()` for both the scaler and PCA components during inference. The `fit_transform()` method computes parameters from input data and applies them, appropriate during training when learning the normalization statistics and principal components from the training set. The `transform()` method applies previously learned parameters without recomputing them, essential during inference to maintain consistency with training data statistics.

If `fit_transform()` were used during inference, the scaler would compute mean and standard deviation from a single frame rather than the training distribution, producing incorrect normalized values. Similarly, PCA would attempt to identify principal components from a single sample, which is mathematically undefined. By using `transform()`, the system applies the scaling parameters and principal components learned from the 5,968 training samples, ensuring that test features occupy the same normalized feature space as training features.

### Confidence Thresholding

The prediction threshold of 60% implements a confidence-based rejection strategy for ambiguous cases. When the maximum probability across all six classes falls below this threshold, the system displays "unknown" rather than showing a low-confidence prediction. This prevents misleading users with unreliable classifications when the input image contains objects outside the training distribution or when lighting conditions create ambiguous visual patterns. The threshold value of 60% was selected through empirical evaluation that balanced prediction confidence against rejection rate, avoiding excessive false positives from low thresholds or excessive rejections from high thresholds.

### Dual Model Display

The system displays predictions from both K-Nearest Neighbors and Support Vector Machine simultaneously, providing multiple perspectives on classification decisions. When both models agree on the predicted class, confidence in the classification increases. When models disagree, it indicates the input falls in an ambiguous region of feature space where decision boundaries differ between the two approaches. This transparency enables users to understand prediction reliability and provides developers with diagnostic information about model behavior during deployment.

---

## üéØ Complete Pipeline Summary

### Data Flow Architecture

The complete system operates through a sequential five-stage pipeline that transforms raw images into real-time classifications. The first stage loads original images from the images directory, where each of six class folders contains unlabeled photographs of waste items. The data augmentation module processes these images, applying geometric and photometric transformations to generate the augmented directory containing 7,460 images representing a four-fold expansion of the original dataset. The feature extraction module loads augmented images, passes them through MobileNetV2 to generate 1,280-dimensional embeddings, appends LAB color statistics to create 1,286-dimensional vectors, applies standardization and PCA dimensionality reduction, and saves the resulting 200-dimensional features to the extracted features directory. The model training modules load these features, perform stratified train-test splitting, train K-Nearest Neighbors and Support Vector Machine classifiers, and serialize the trained models to disk. The real-time inference module loads trained models and preprocessing artifacts, captures webcam frames continuously, extracts features using identical processing steps, generates predictions with confidence thresholding, and displays annotated results in real-time.

### Critical Design Decisions

The transition from traditional computer vision feature extraction to deep learning embeddings represents the most significant architectural decision in this project. The previous pipeline utilized handcrafted features including Histogram of Oriented Gradients for shape representation, Local Binary Patterns for texture analysis, Gray-Level Co-occurrence Matrix statistics for texture properties, and Bag of Visual Words for local feature aggregation. While this approach provided interpretable features based on computer vision principles, it required extensive domain expertise to engineer effective descriptors and achieved 86.86% and 88.54% accuracy for K-Nearest Neighbors and Support Vector Machine respectively.

The current MobileNetV2-based approach leverages transfer learning from ImageNet to extract learned representations that capture hierarchical visual patterns discovered through large-scale supervised training. This eliminates the need for manual feature engineering while dramatically improving classification accuracy to 99.73% and 99.87%, representing approximately 13 percentage points improvement. The success of transfer learning in this application demonstrates that visual patterns learned from natural images generalize effectively to waste classification despite the domain difference, as fundamental concepts such as material textures, object shapes, and color properties remain consistent.

The selection of MobileNetV2 specifically addresses the computational constraint of CPU-only inference. Standard convolutional architectures such as ResNet or VGG employ regular convolutions that require substantial computational resources, making real-time inference impractical on laptops without GPU acceleration. MobileNetV2 utilizes depthwise separable convolutions that factorize standard convolutions into depthwise (spatial filtering) and pointwise (channel mixing) operations, reducing computational cost by approximately an order of magnitude while maintaining strong representational capacity. This architectural innovation enables practical deployment on standard hardware while achieving state-of-the-art accuracy.

The retention of LAB color statistics alongside CNN embeddings reflects the recognition that color information provides complementary discriminative power for waste classification. While MobileNetV2 learns color-sensitive features through its convolutional filters, explicitly encoding color distribution statistics ensures that this information is prominently represented in the final feature vector. For classes where color is a primary distinguishing characteristic, such as brown cardboard versus clear plastic or green glass versus silver metal, this explicit color encoding enhances classification performance.

The PCA dimensionality reduction from 1,286 to 200 dimensions serves multiple purposes. First, it removes redundant information and noise from the feature representation, improving classifier generalization by focusing on the most discriminative patterns. Second, it reduces computational requirements during classifier training and inference by operating on a more compact representation. Third, it addresses the curse of dimensionality that affects distance-based methods such as K-Nearest Neighbors in very high-dimensional spaces. The retention of 75.92% variance indicates that the 200 principal components capture the vast majority of meaningful variation while discarding noise and redundant patterns.

The implementation of both K-Nearest Neighbors and Support Vector Machine classifiers enables comparative analysis of complementary approaches. K-Nearest Neighbors provides an intuitive non-parametric method that requires no training time and adapts naturally to complex decision boundaries through local voting. Support Vector Machine learns a global decision function optimized for maximum margin, providing stronger theoretical guarantees about generalization performance. The minimal performance difference between the two approaches (99.73% versus 99.87%) indicates that the MobileNetV2 features provide such strong discriminative power that even simple classifiers achieve near-perfect accuracy, validating the effectiveness of the deep learning feature extraction stage.

### Performance Analysis

The system achieves exceptional classification accuracy that positions it as production-ready for real-world deployment. The K-Nearest Neighbors classifier accuracy of 99.73% indicates that only 4 misclassifications occur per 1,474 test samples. The Support Vector Machine classifier accuracy of 99.87% indicates only 2 misclassifications per 1,474 test samples. These error rates are remarkably low considering the inherent ambiguity in some waste items, such as wax-coated paper that shares properties of both paper and plastic, or composite materials that contain multiple waste types.

The inference performance of 5-10 frames per second enables practical real-time operation, though with noticeable latency compared to GPU-accelerated systems that typically achieve 30-60 frames per second. This frame rate is sufficient for point-and-classify applications where users hold items stationary for a few seconds to obtain classification results, though it would be insufficient for high-speed sorting applications on conveyor belts. The primary computational bottleneck is the MobileNetV2 forward pass, which executes 53 layers of convolution and normalization operations on CPU. Future optimization could explore model quantization or pruning techniques to further reduce inference latency.

The class-wise performance analysis reveals that all categories achieve above 98% accuracy, indicating no systematic weaknesses in the classification system. The highest accuracy typically occurs for glass and metal due to their distinctive visual properties (reflectivity, smoothness, color uniformity), while the lowest accuracy occurs for trash due to its heterogeneous composition and smaller training set size. However, even the trash category achieves over 98% accuracy, demonstrating robust performance across all classes.

### Practical Applications

This waste classification system addresses a critical environmental challenge by enabling automated sorting that reduces contamination rates in recycling streams. Manual sorting by human workers is labor-intensive, expensive, and subject to fatigue-related errors that degrade sorting accuracy over extended shifts. Automated optical sorting systems using machine learning can maintain consistent accuracy without fatigue while processing higher volumes of material.

The system could be deployed in multiple contexts including residential recycling education applications where users photograph items to receive disposal guidance, commercial sorting facilities that process mixed recyclables, and smart waste bins that automatically route items to appropriate compartments. The high accuracy and CPU-feasible inference make the system accessible for deployment on embedded devices or edge computing platforms without requiring cloud connectivity or expensive GPU hardware.

---

## üìä Performance Summary

### Dataset Statistics

| Category | Original Images | Augmented Images | Percentage of Dataset |
|----------|----------------|------------------|---------------------|
| Cardboard | 259 | 988 | 13.2% |
| Glass | 401 | 1,540 | 20.6% |
| Metal | 328 | 1,260 | 16.9% |
| Paper | 476 | 1,796 | 24.1% |
| Plastic | 386 | 1,452 | 19.5% |
| Trash | 110 | 424 | 5.7% |
| **Total** | **1,960** | **7,460** | **100%** |

### Model Performance Comparison

| Model | Accuracy | Performance Gain |
|-------|----------|-----------------|
| **Traditional CV Features + KNN** | 86.86% | Baseline |
| **Traditional CV Features + SVM** | 88.54% | Baseline |
| **MobileNetV2 + KNN** | **99.73%** | **+12.87%** |
| **MobileNetV2 + SVM** | **99.87%** | **+11.33%** |

### Technical Specifications

| Component | Specification |
|-----------|--------------|
| CNN Architecture | MobileNetV2 (pretrained on ImageNet) |
| Input Resolution | 224 √ó 224 √ó 3 (RGB) |
| Raw Feature Dimension | 1,286 (1,280 CNN + 6 color) |
| PCA Dimension | 200 |
| Variance Retained | 75.92% |
| Training Set Size | 5,968 images (80%) |
| Test Set Size | 1,492 images (20%) |
| Inference Speed | 5-10 FPS (CPU) |

---

## üöÄ Installation and Usage

### Dependencies

The system requires Python 3.7 or higher along with the following packages:

```bash
pip install tensorflow opencv-python numpy scikit-learn scikit-image joblib matplotlib seaborn tqdm
```

For CPU-only deployment, TensorFlow automatically configures for CPU inference without requiring additional setup. For GPU acceleration, install the GPU-enabled TensorFlow version following official documentation.

### Directory Structure Setup

Ensure the following directory structure exists before running the pipeline:

```
MSI_PROJECT/
‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ cardboard/
‚îÇ   ‚îú‚îÄ‚îÄ glass/
‚îÇ   ‚îú‚îÄ‚îÄ metal/
‚îÇ   ‚îú‚îÄ‚îÄ paper/
‚îÇ   ‚îú‚îÄ‚îÄ plastic/
‚îÇ   ‚îî‚îÄ‚îÄ trash/
‚îî‚îÄ‚îÄ (remaining directories created automatically)
```

Place original images in the appropriate class folders within the images directory. Each image filename can be arbitrary as the system derives labels from parent folder names.

### Pipeline Execution

Execute the pipeline components sequentially as follows:

**Step 1: Data Augmentation**
```bash
python data_augmentation.py
```

This generates the augmented directory containing 7,460 images. Execution time depends on original dataset size but typically completes within minutes.

**Step 2: Feature Extraction**
```bash
python feature_extraction_mobilenet.py
```

This downloads MobileNetV2 weights on first execution (approximately 14MB), extracts CNN embeddings for all augmented images, and saves processed features to the extracted features directory.

**Step 3: Model Training**
```bash
python knn_training.py
python svm_training.py
```

These scripts train both classifiers and generate comprehensive visualization plots saved as PNG files. Training completes within seconds to minutes depending on dataset size.

**Step 4: Real-Time Inference**
```bash
python input.py
```

This launches the webcam interface displaying live predictions. Press 'q' to exit the application.

### Expected Outputs

Upon successful pipeline execution, the following files will be generated:

- `augmented/` directory containing 7,460 augmented images organized by class
- `extracted_features/X.npy` containing 200-dimensional PCA-reduced features
- `extracted_features/X_raw.npy` containing 1,286-dimensional raw embeddings
- `extracted_features/y.npy` containing numeric class labels
- `extracted_features/scaler.pkl` containing fitted StandardScaler model
- `extracted_features/pca.pkl` containing fitted PCA model
- `extracted_features/class_map.txt` containing class name to index mapping
- `knn_model.pkl` containing trained K-Nearest Neighbors classifier
- `svm_model.pkl` containing trained Support Vector Machine classifier
- `knn_visualization.png` containing six-panel analysis plots
- `svm_visualization.png` containing six-panel analysis plots

---

## üîß Troubleshooting

### Common Issues and Solutions

**Issue: "No images found in augmented/"**

This error indicates the augmentation step has not been executed. Run `python data_augmentation.py` to generate the augmented dataset before proceeding to feature extraction.

**Issue: "Shape mismatch during model training"**

This typically occurs when feature extraction has been run with different parameters than expected. Delete the `extracted_features/` directory and re-run `python feature_extraction_mobilenet.py` to regenerate features with correct dimensions.

**Issue: "Cannot open camera" during inference**

This indicates the webcam is unavailable or already in use by another application. Verify that the camera is connected and not being accessed by other software. Alternatively, modify the camera index in `input.py` from `cv2.VideoCapture(0)` to `cv2.VideoCapture(1)` or higher values to access external cameras.

**Issue: Low classification accuracy (<90%)**

This suggests potential issues with data quality or consistency. Verify that images are clear and well-lit, that class folders are correctly labeled, and that the augmented dataset contains sufficient samples per class (minimum 100-200 images recommended per category).

**Issue: All predictions showing "unknown" during inference**

This indicates feature distribution mismatch between training and inference. Ensure that the camera provides RGB color images rather than grayscale, that lighting conditions are reasonable, and that the confidence threshold is not set excessively high. Consider reducing the threshold from 0.60 to 0.40 in `input.py` for testing purposes.

**Issue: Memory errors during feature extraction**

This occurs when batch processing exceeds available RAM. Reduce the `BATCH_SIZE` parameter in `feature_extraction_mobilenet.py` from 16 to 8 or lower to decrease memory consumption at the cost of slightly longer processing time.

---

## üìà Future Enhancements

### Potential Improvements

Several avenues exist for further system enhancement. First, fine-tuning the MobileNetV2 network on the waste classification task rather than using fixed pretrained features could improve accuracy by adapting learned representations to domain-specific patterns. This requires careful regularization to prevent overfitting given the relatively small dataset. Second, ensemble methods that combine predictions from multiple models using voting or stacking could potentially push accuracy beyond 99.9% by leveraging complementary strengths of different architectures. Third, active learning strategies could identify the most informative samples for human labeling, enabling targeted dataset expansion that maximizes accuracy gains per labeled example. Fourth, multi-modal approaches incorporating metadata such as object weight, size, or acoustic properties could complement visual classification for difficult cases. Fifth, explainability techniques such as Grad-CAM could visualize which image regions contribute most to classification decisions, increasing user trust and enabling failure analysis.

From a deployment perspective, model compression through quantization or knowledge distillation could reduce inference latency while maintaining accuracy, enabling real-time processing at higher frame rates. Integration with robotic sorting systems would require additional engineering for object localization and grasping, transforming the current classification system into a complete automated sorting solution. Cloud deployment with edge preprocessing could enable centralized model updates and performance monitoring across distributed installations.

---

## üìù Conclusion

This waste classification system demonstrates the effectiveness of transfer learning and modern deep learning architectures for practical computer vision applications. By leveraging MobileNetV2 pretrained on ImageNet, the system achieves 99.73% and 99.87% accuracy for K-Nearest Neighbors and Support Vector Machine classifiers respectively, representing a substantial improvement over traditional computer vision approaches. The careful attention to pipeline consistency between training and inference, the strategic use of data augmentation to address limited training data, and the selection of computationally efficient architectures suitable for CPU inference collectively enable a production-ready system that balances accuracy, speed, and deployment feasibility.

The project illustrates several key principles of applied machine learning including the importance of data quality and quantity, the power of transfer learning for leveraging pretrained knowledge, the value of multiple classifier comparison for robust evaluation, and the critical need for exact pipeline replication between development and deployment. The exceptional accuracy achieved suggests readiness for real-world deployment in recycling education and automated sorting applications, with potential to contribute meaningfully to waste management efficiency and environmental sustainability.
